# **[Evaluation of Production Serverless Computing Environments](http://dsc.soic.indiana.edu/publications/Evaluation_of_Production_Serverless_Computing_Environments.pdf)**

## Abstract
Serverless computing provides a small runtime container to execute lines of codes without infrastructure management which is similar to Platform as a Service (PaaS) but a functional level. Amazon started the event-driven compute named Lambda functions in 2014 with a 25 concurrent limitation, but it now supports at least a thousand of concurrent invocation to process event messages generated by resources like databases, storage and system logs. Other providers, i.e., Google, Microsoft, and IBM offer a dynamic scaling manager to handle parallel requests of stateless functions in which additional containers are provisioning on new compute nodes for distribution. However, while functions are often developed for microservices and lightweight workload, they are associated with distributed data processing using the concurrent invocations. We claim that the current serverless computing environments can support dynamic applications in parallel when a partitioned task is executable on a small function instance. We present results of throughput, network bandwidth, a file I/O and compute performance regarding the concurrent invocations. We deployed a series of functions for distributed data processing to address the elasticity and then demonstrated the differences between serverless computing and virtual machines for cost efficiency and resource utilization.

**Category: performance evaluation, benchmarking, serverless computing, functions-as-a-service, public cloud infrastructure.**

## Problem 
Serverless computing is a cloud computing paradigm, a cloud service that enables run stateless functions on ephemeral containers with small resource allocation. Containers are more lightweight than virtual machines, which means that we can start and destroy them more quickly too once virtual machines need time to scale with system settings (like an instance type, a base image, a network configuration, and a storage option), while containers only need to load its container image - and that's only one of the container's advantages against virtual machines. A problem related with serverless environment is that we don't know what applications are viable with their current state. For example, does distributed data processing applications viable in those platforms?

## Proposal
The paper evaluates serverless computing environments invoking functions in parallel to demonstrate the performance and throughput of serverless computing for distributed data processing through four popular public cloud serverless providers - AWS Lambda, Google Cloud Function, Microsoft Azure Functions, and IBM OpenWhisk. Four main types of evaluation are made.

- Performance comparison of CPU, memory, and disk intensive functions running in between a sequential and a concurrent invocation which helps understanding performance bottlenecks and function behaviors on serverless computing environments. 
- Throughput measure of a set of event handlers including HTTP, database and storage which may indicate a maximum size of dequeuing event messages because functions are triggered by these common handlers supported by each serverless provider. 
- Continuous development and integration teste with source code and function configuration changes while concurrent functions are running. 
- Comparisons between Infrastructure as a Service (IaaS) and Function as a Service (FaaS) using experiments on big data and deep learning applications and the latest features offered by each serverless computing environment.

## Evaluation
Serverless computing environments were evaluated on the throughput of concurrent invocation, CPUs, response time for dynamic workload, runtime overhead, and a temporary directory I/O performance. The evaluation also compares cost-effectiveness, event
trigger throughput, and features using a set of functions written by supported runtimes. 

#### Concurrent Function Throughput
- **Overview**: Function throughput indicates concurrent processing since it tells how many function instances are supplied to deal with heavy requests. To evaluation concurrent function throughput, they created thousands of functions with identical logic but different name and treat them like invoking a single function in parallel. Under a concurrent function invocation from 500 to 10000, they watched the throughput per second through all four platforms.
- **Function**: The function's business logic was not informed.
- **Metrics**: 
  - Throughput per second, number of functions invocation satisfied per second.
- **Factor**: 
  - From 500 to 10000 concurrent invocations.
- **Results**: 
  - Amazon Lambda in average generates about 400 throughputs per second, reaching its maximum function throughput at a small concurrent invocations number of 1000. 
  - IBM OpenWhisk and Microsoft Azure Functions show similar behavior in reaching its best throughput at 2000 invocations and decreasing slowly over increased concurrent invocations. 
  - Google Functions indicates slow but steady increase of throughput over increased invocations and reachs its maximum at 10000 concurrent invocations, having function throughput greater than IBM and Azure.

#### Concurrency for CPU Intensive Workload
- **Overview**: Multiplying two-dimensional array requires mostly compute operations without consuming other resources, which makes that a viable choice of function's business logic to stress CPU resources on a function instance with concurrent invocations. To evaluation concurrency for CPU intensive workload, they implemented in javascript a matrix multiplication function and invoke them under one and one hundred concurrent invocations.
- **Function**: A matrix multiplication function written in a JavaScript.
- **Metrics**: 
  - Elapsed time, the execution time of a function.
  - GFLOPS (Giga Floating Point Operations Per Second) per function.
  - TFLOPS (Tera Floating Point Operations Per Second) in total of 3000.
- **Factor**: 
  - 1 and 100 concurrent invocations.
- **Results**: 
  - The results for non-parallel invocation are consistent whereas results with 100 invocations show a overhead between 28% and 4606% over the total execution time. It also implies that more than one invocation was assigned to a single instance, which means that two cpu-intensive function invocations may take twice longer by sharing CPU time in half. 
  - In the comparison of the total of GFLOPS and TFLOPS, AWS Lambda generates 4-8 and 5-7 times faster compute speed than others, respectively. Azure Functions, IBM OpenWhisk, and Google Functions are in an early stage of development and is expected that the allocated compute resource will be more comparable when the services are fully mature.

#### Concurrency for Disk Intensive Workload
- **Overview**: Serverless functions have small sized writable temporary directory useful for many purposes such as storing extra libraries, tools, and intermediate data files. To evaluate concurrency for disk intensive workload, a function that writes and reads in that temporary directory was implemented and they measure the I/O performance with one and one hundred concurrent invocations.
- **Function**: A function that writes and reads files on the temp directory to stress a file I/O.
- **Metrics**:
  - Elapsed time, the execution time of a function.
- **Factor**: 
  - 1 and 100 concurrent invocations.
- **Results**: 
  - Microsoft Functions fail to complete function invocations within the execution time limit of 5 minutes.
  - Read results with 100 invocations show an overhead of 91% on Amazon Lambda, 145% on Google Functions and 338% on IBM OpenWhisk whereas write results with 100 invocations show an overhead of 110% on Amazon Lambda, 164% on Google Functions and 1472% on IBM OpenWhisk.
  - Notoriously, writing a file between 1 and 100 concurrent invocations is slightly worse compared to reading. 
  - Writing speed on Amazon Lambda is 11 to 78 times faster than Google and IBM when 100 concurrent invocations are made.

#### Concurrency for Network Intensive Workload
- **Overview**: Processing dataset from dynamic applications such as big data and machine learning often incur significant performance degradation in congested networks due to heavy transactions of file uploading and downloading. To evaluate that, they implemented a function which requests 100 megabytes sized data from object storage on each service provider. Thus, using this function, a hundred concurrent invocations create network traffic in total of 10 gigabytes. 
- **Function**: A function which requests 100 megabytes sized data.
- **Metrics**:
  - Elapsed time, the execution time of a function.
- **Factor**: 
  - 1 and 100 concurrent invocations.
- **Results**: 
  - Google Functions has a minimal overhead between 1 and the 100 concurrency level while Amazon Lambda is four times faster in loading data from Amazon object storage (S3) than Google object storage.
  - Microsoft Azure Functions fails to get access of data from its blob storage at 100 concurrency level and they suspect it might be caused by the experimental runtime.

#### Elasticity
- **Overview**: Dynamic application performing latency-sensitive workloads needs elastic provisioning of function instances otherwise overhead and failure would be observed during the processing of workloads. To evaluate elasticity they implemented a function that takes less than 100 milliseconds and that's all (they did not get into function details). The number of concurrent invocation was random numbers ranging from 10 to 90 over time resulting in about 10 thousands of the total invocations within a minute.
- **Function**: A function that takes less than 100 milliseconds running in NodeJS runtime. Business logic was not informed.
- **Metrics**:
  - Delays of instantiating new instances (cold start): obseved when existing function instances are overloaded and new instances are added slowly which may cause performance degradation in the entire invoked functions.
  - Total number of instances created: observed when a workload jumps to higher than the point that existing instances can handle and the increased number of function instances stay for a while to process future requests.
  - Execution time, the execution time of a function invocation.
- **Factor**: 
  - from 10 to 90 concurrent invocations.
- **Results**: 
  - Amazon and Google support elasticity well. Their 99th percentile of the function execution time is below 100 and 200 milliseconds, respectively.
  - Both IBM and Azure show significant overhead comparing the 99th percentile of execution time. The IBM overhead is at least two times bigger than the others at this percentile and Azure has an overhead of at least eight times bigger than IBM.

#### Continuous Deployment and Integration
- **Overview**: Development and Operations (DevOps) paradigm is applied to serverless functions to enable continuous delivery and integration while functions are in action. So, they evaluated the deployment and integration of functions by changing the source code before the first 200 invocations and an update of configurations within the next 200 invocations thus different behaviors should be observed close to these timelines.
- **Metrics**:
  - Function instances: Looking into existing, new and failed ones. 
- **Factor**: 
  - 500 function executions in total.
  - 10 concurrent invocations.
- **Results**: 
  - Amazon has a certain period to replace an existing deployment to new one. There are exstisting instances between 200 and 400 invocations, which is a indicative that previous function instances continued to process several event messages although a new piece of source code is published before the 200 invocations.
  - Google and IBM refreshed the entire instances when there is a new deployment although Google failed to multiple requests during the update.
  - Microsoft Azure shows a similar behavior but it is not clearly visible in the figure due to a small number of instances.

#### Serverless versus Virtual Machine
- **Overview**: Serverless computing aims to provide dynamic compute resources for lightweight functions without administrations and offers cost-efficient solutions like pay-as-you-go whereas virtual machines have offered multiple options to scale compute resources with machine types, network bandwidth and storage performance to meet the expectation of performance requirements of a given workload which requires optimal capacity planning and system management. In this context, the paper introduces experimental results to understand the differences between these two computing models and a cost comparison to explain their cost effectiveness.
- **Metrics**: 
  - RAM Memory.
  - Cost per Second.
  - Elapsed Second.
  - Total Cost.
- **Factor**: 
  - Container.
  - Virtual Machine.
- **Results**:
  - Sequential and continuing function on serverless computing would not be a good choice in terms of cost-savings although it is still a simple way of deploying a function as a service with a minimal infrastructure management. 
  - Dynamic concurrent invocations on serverless computing will save cost against overloaded virtual machines when a number of event messages spikes.

#### Trigger Comparison
- **Overview**: In order to measure a trigger throughput to indicate the maximum number of processing event messages in parallel, three types of triggers are selected; HTTP, database and object storage triggers. They did not share further detail about their experiment, but they did tell that Azure and Google Cloud database trigger were not compared as they do not have a direct trigger available to their respective functions.
- **Metrics**:
  - Number of functions processed per second
- **Factor**: 
  - HTTP, database and object storage triggers.
- **Results**:
  - Triggers in AWS Lambda show that the median throughput of the HTTP trigger is 55.7 functions per second and the object storage has the 25.16 functions per second median throughput.
  - The database trigger in AWS Lambda has throughput of 864.60 functions per second which is about 32 times of object storage throughput and 15 times of HTTP trigger throughput.
  - Microsoft Azure has the highest number of 142 invocations per second whereas Google Functions shows the least throughput as they invoke very less number of functions per second.
  - All serverless providers show a linear pattern of function invocation when the number of requests is increased.
  - There is no signal of any degradation of performance in handling massive requests up to 3000 concurrent invocations and can be concluded that the increase in invocation does not affect the performance.
  - They didn't make commentaries about OpenWhisk, but it seems that it had bad results for object storage or that parameter was not tested.

#### Feature Comparison
- **Overview**: The feature comparison helps readers of this paper understand the underlying system level information of the serverless platform. To do that, a table was used to compare options of Runtime language, Trigger, Price per Memory, Price per Execution, Free Tier, Maximum Memory, Container OS, Container CPU Info, Temp Directory (Path), Execution Timeout and Code Size Limit of each serverless provide evaluated. 
- **Results**:  
  - AWS Lambda offers a wide range of trigger endpoints compared to the other cloud providers and has cost of usage of serverless function based on two metrics: the number of functions invocation and the time taken by a function to execute and complete paired with an amount of memory in size of gigabytes allocated.
  - All providers have similar pricing tables but IBM openWhisk does not charge the number of invocations whereas the other providers do charge.
  - Google upscales in terms of memory as it provides maximum of 2 GB of memory to run a serverless function. It also outperforms in terms of providing maximum execution timeout of 9 minutes which would be helpful for long running jobs.
  - IBM OpenWhisk has the container which can provided the best clock speed of 2100 *4 MHz.

#### Language Support
- **Overview**: Each serverless provider supports different programming languages. This evaluation compares which languages are available in each provider and compares each language runtime overhead price in each serverless provider, guiding to understand trade-offs of choosing a language.
- **Results**:
  - As an interpreted language, JavaScript runtime environment is present in all of the providers while Python is mostly supported of them. Compiled languages such as Java and C# are also supported and is assumed that serverless providers intends to extend language support in the future. 
  - There is a runtime overhead between the supported languages in each provider that impacts runtime costs in seconds. This overhead may be negligible, but in some cases, it might be sensitive enough to choose a language runtime since there are timeouts in executing a function. For instance, runtime overhead in AWS is negligible, C# in Azure creates the least overhead among other runtimes, Python in IBM OpenWhisk shows the least standard deviation and Node.js environment is a better choice in Google Functions. 

## Conclusion
The paper claim that the current serverless computing environments are able to support dynamic applications in parallel as said at Abstract and questioned in Problem, and concludes that serverless computing functions are able to process distributed data applications by quickly provisioning additional compute resources on multiple containers. 

Results show that the elasticity of Amazon Lambda exceeds others regarding CPU performance, network bandwidth, and I/O throughput when concurrent function invocations are made for dynamic workloads. Overall, serverless computing is able to scale relatively well to perform distributed data processing if a divided task is small enough to execute on a function instance with 1.5GB to 3GB memory limit and 5 to 10 minute execution time limit. 

It also indicates that serverless computing would be more cost-effective than processing on traditional virtual machines because of the almost zero delay on boot up new instances for additional function invocations and a charging model only for the execution time of functions instead of paying for idle time of machines.

## Review

## Related Work
1. [Return of the runtimes: Rethinking the language runtime system for the cloud 3.0 era] it discusses the programming language runtime on serverless computing.
2. [Faaster, better, cheaper: The prospect of serverless scientific computing and hpc] argue the possible use cases of serverless computing using scientific computing applications.
3. [Zappa, Serverless python web services] a python based serverless powered on Amazon Lambda that keeps functions in warm state by poking at a regular interval.
4. [Serverless computing: Design, implementation, and performance] a function latency comparison of production serverless computing environments.

## Links
- [IEEEXplore document](https://ieeexplore.ieee.org/document/8457830)
- [ResearchGate publication](https://www.researchgate.net/publication/324362882_Evaluation_of_Production_Serverless_Computing_Environments)
- [Digital Science Center, Indiana University Bloomington (pdf)](http://dsc.soic.indiana.edu/publications/Evaluation_of_Production_Serverless_Computing_Environments.pdf)
- [Github repository](https://github.com/lee212/FaaS-Evaluation)
