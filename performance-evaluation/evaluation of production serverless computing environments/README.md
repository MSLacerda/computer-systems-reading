# **[Evaluation of Production Serverless Computing Environments](http://dsc.soic.indiana.edu/publications/Evaluation_of_Production_Serverless_Computing_Environments.pdf)**

## Abstract
Serverless computing provides a small runtime container to execute lines of codes without infrastructure management which is similar to Platform as a Service (PaaS) but a functional level. Amazon started the event-driven compute named Lambda functions in 2014 with a 25 concurrent limitation, but it now supports at least a thousand of concurrent invocation to process event messages generated by resources like databases, storage and system logs. Other providers, i.e., Google, Microsoft, and IBM offer a dynamic scaling manager to handle parallel requests of stateless functions in which additional containers are provisioning on new compute nodes for distribution. However, while functions are often developed for microservices and lightweight workload, they are associated with distributed data processing using the concurrent invocations. We claim that the current serverless computing environments can support dynamic applications in parallel when a partitioned task is executable on a small function instance. We present results of throughput, network bandwidth, a file I/O and compute performance regarding the concurrent invocations. We deployed a series of functions for distributed data processing to address the elasticity and then demonstrated the differences between serverless computing and virtual machines for cost efficiency and resource utilization.

**Category: performance evaluation, benchmarking, serverless computing, functions-as-a-service, public cloud infrastructure.**

## Problem 
Serverless computing is a cloud computing paradigm, a cloud service that enables run stateless functions on ephemeral containers with small resource allocation. Containers are more lightweight than virtual machines, which means that we can start and destroy them more quickly too, once virtual machines need time to scale with system settings like an instance type, a base image, a network configuration, and a storage option, while containers only need to load its container image. That's only one, among others, of the container's advantages against virtual machines. This paradigm seems promising, but we don't know all viable applications with the current state of commercial serverless platforms. Applications that do distribute data processing using concurrent invocations are an example. So, said that, does the current serverless computing environments able to support dynamic applications in parallel when a partitioned task is executable on a small function instance?

## Proposal
The paper evaluates serverless computing environments invoking functions in parallel to demonstrate the performance and throughput of serverless computing for distributed data processing through four popular public cloud serverless providers - AWS Lambda, Google Cloud Function, Microsoft Azure Functions, and IBM OpenWhisk. Four main types of evaluation are made.

- Performance comparison of CPU, memory, and disk intensive functions running in between a sequential and a concurrent invocation which helps understanding performance bottlenecks and function behaviors on serverless computing environments. 
- Throughput measure of a set of event handlers including HTTP, database and storage which may indicate a maximum size of dequeuing event messages because functions are triggered by these common handlers supported by each serverless provider. 
- Continuous development and integration tested with source code and function configuration changes while concurrent functions are running. 
- Comparisons between Infrastructure as a Service (IaaS) and Function as a Service (FaaS) using experiments on big data and deep learning applications and the latest features offered by each serverless computing environment.

## Evaluation
Serverless computing environments were evaluated on the throughput of concurrent invocation, CPUs, response time for dynamic workload, runtime overhead, and a temporary directory I/O performance. The evaluation also compares cost-effectiveness, event
trigger throughput, and features using a set of functions written by supported runtimes. 

#### Concurrent Function Throughput
- **Overview**: Function throughput indicates concurrent processing since it tells how many function instances are supplied to deal with heavy requests. They created thousands of functions with identical logic but different name and treat them like invoking a single function in parallel. Under a  concurrent function invocation from 500 to 10000, they watched the throughput per second  through all four platforms.
- **Function**: The function's business logic was not informed.
- **Metrics**: Throughput per second
- **Factor**: From 500 to 10000 concurrent invocations.
- **Results**: 
  - Amazon Lambda in average generates about 400 throughputs per second, reaching its maximum function throughput at a small concurrent invocations number of 1000. 
  - IBM OpenWhisk and Microsoft Azure Functions show similar behavior in reaching its best throughput at 2000 invocations and decreasing slowly over increased concurrent invocations. 
  - Google Functions indicates slow but steady increase of throughput over increased invocations and reachs its maximum at 10000 concurrent invocations, having function throughput greater than IBM and Azure.

#### Concurrency for CPU Intensive Workload
- **Overview**: Multiplying two-dimensional array requires mostly compute operations without consuming other resources, which makes that a viable choice of function's business logic to stress CPU resources on a function instance with concurrent invocations. 
- **Function**: A matrix multiplication function written in a JavaScript.
- **Metrics**: 
  - Elapsed time, the execution time of a function.
  - GFLOPS (Giga Floating Point Operations Per Second) per function.
  - TFLOPS (Tera Floating Point Operations Per Second) in total of 3000.
- **Factor**: 1 and 100 concurrent invocations.
- **Results**: 
  - The results for non-parallel invocation are consistent whereas results with 100 invocations show a overhead between 28% and 4606% over the total execution time. It also implies that more than one invocation was assigned to a single instance, which means that two cpu-intensive function invocations may take twice longer by sharing CPU time in half. 
  - In the comparison of the total of GFLOPS and TFLOPS, AWS Lambda generates 4-8 and 5-7 times faster compute speed than others, respectively. Azure Functions, IBM OpenWhisk, and Google Functions are in an early stage of development and is expected that the allocated compute resource will be more comparable when the services are fully mature.

#### Concurrency for Disk Intensive Workload
#### Concurrency for Network Intensive Workload
#### Elasticity
#### Continuous Deployment and Integration
#### Serverless versus Virtual Machine
#### Trigger Comparison
#### Feature Comparison
#### Language Support

## Conclusion
The paper claim that the current serverless computing environments are able to support dynamic applications in parallel as said at abstract and asked in problems and concludes that serverless computing functions are able to process distributed data applications by quickly provisioning additional compute resources on multiple containers. 

Results show that the elasticity of Amazon Lambda exceeds others regarding CPU performance, network bandwidth, and I/O throughput when concurrent function invocations are made for dynamic workloads. Overall, serverless computing is able to scale relatively well to perform distributed data processing if a divided task is small enough to execute on a function instance with 1.5GB to 3GB memory limit and 5 to 10 minute execution time limit. 

It also indicates that serverless computing would be more cost-effective than processing on traditional virtual machines because of the almost zero delay on boot up new instances for additional function invocations and a charging model only for the execution time of functions instead of paying for idle time of machines.

## Review

## Related Work
1. [Return of the runtimes: Rethinking the language runtime system for the cloud 3.0 era] it discusses the programming language runtime on serverless computing.
2. [Faaster, better, cheaper: The prospect of serverless scientific computing and hpc] argue the possible use cases of serverless computing using scientific computing applications.
3. [Zappa, Serverless python web services] a python based serverless powered on Amazon Lambda that keeps functions in warm state by poking at a regular interval.
4. [Serverless computing: Design, implementation, and performance] a function latency comparison of production serverless computing environments.

## Links
- [IEEEXplore document](https://ieeexplore.ieee.org/document/8457830)
- [ResearchGate publication](https://www.researchgate.net/publication/324362882_Evaluation_of_Production_Serverless_Computing_Environments)
- [Digital Science Center, Indiana University Bloomington (pdf)](http://dsc.soic.indiana.edu/publications/Evaluation_of_Production_Serverless_Computing_Environments.pdf)
- [Github repository](https://github.com/lee212/FaaS-Evaluation)
